---
title: "Diabetes Health Indicators - Modeling"
author: "Brock Akerman"
format: html
bibliography: references.bib
csl: apa.csl
---

```{r, library load, warning=FALSE, output=FALSE}
library(tidyverse)
library(dplyr)
library(caret)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(randomForest)
```

### Diabetes Health Indicators Modeling

After conducting an exploratory data analysis (EDA), our objective is to make predictions, uncover patterns within the data, and identify the most suitable model to describe the dataset. Key variables of interest include high cholesterol, high blood pressure, physical activity, fruit and vegetable consumption, body mass index, age, income, and education. Recognizing potential interactions between these variables, we will incorporate several models that account for hypothesized interactions.

We will fit logistic regression models, classification tree models, and random forest models. Following this, we will identify the best-performing model from each category and compare their performance on the test data to determine the overall best model.

```{r, read-in data, output=FALSE, warning=FALSE, error=FALSE}
#Read in the dataset
diabetes_data_raw <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")
```

```{r, factorize_vars,echo=FALSE}
#Factorize the binary variables for better downstream analysis and remove BMI observations 50
diabetes_data_clean <- diabetes_data_raw %>%
  mutate_at(vars(Diabetes_binary, HighBP, HighChol, CholCheck, Smoker, 
                 Stroke, HeartDiseaseorAttack, PhysActivity, Fruits, 
                 Veggies, HvyAlcoholConsump, AnyHealthcare, NoDocbcCost, 
                 DiffWalk, Sex), as.factor) %>%
  filter(BMI < 50)
```

### Data Preparation for Modeling

Using the caret package syntax, I have split the data into train and test at a 70:30 ratio respectively.  

```{r, split_data}
#Split the data using caret package syntax
set.seed(13579)
diabetesIndex <- createDataPartition(diabetes_data_clean$Diabetes_binary, p = 0.7, list = FALSE)
diabetes_train <- diabetes_data_clean[diabetesIndex, ] #Count = 175,794
diabetes_test <- diabetes_data_clean[-diabetesIndex, ] #Count = 75,339

#I had some problems downstream while planning and executing code to build models. R does not like when I run train() on a response variable with strictly numeric factors.  The following code corrects that issue by changing the response values from "0" and "1" to "X0" and "X1".  Henceforth, "X0" will indicate patients who do not have a diabetes diagnosis while an "X1" will affirm a diabetes diagnosis. 

# Rename levels to valid variable names
levels(diabetes_train$Diabetes_binary) <- make.names(levels(diabetes_train$Diabetes_binary))
levels(diabetes_test$Diabetes_binary) <- make.names(levels(diabetes_test$Diabetes_binary))
```

Caret contains a trainControl() function that allows the setting of modeling parameters

```{r, cv_control}
# Define the control using a 5-fold cross-validation
train_control <- trainControl(method = "cv", number = 5, summaryFunction = mnLogLoss, classProbs = TRUE)
#NOTES:  classProbs has to be TRUE for mnLogLoss to work.
#Source:  https://stackoverflow.com/questions/59669490/error-with-caret-and-summaryfunction-mnlogloss-columns-consistent-with-lev

#tuningParam <- 

```

### Log Loss and Logistic Regression Modeling

Log loss is preferred when datasets have binary response variables over other measurement metrics like accuracy because of the penalty it places on incorrect predictions.  Log Loss compares prediction probabilities to binary output; and evaluates probability estimates which make for a more robust measurement method over accuracy for probabilistic models. 

Using linear modeling might not be appropriate for this data.  Logistic regression modeling would be a better alternative.  In the plot below, notice that the linear regression line does not fit our data well for a variable BMI.  The entire linear regression line completely avoids the positive diabetes diagnosis points.

```{r, jitter_plot, warning=FALSE}
ggplot(diabetes_data_clean, aes(x = BMI, y = Diabetes_binary)) + 
  geom_jitter() + 
  geom_smooth(method = "lm", aes(group = 1))
```

### Logistic Regression Models

```{r, mod_train}
# Base LogReg Model with no interactions
set.seed(13579)
base_model_LogReg <- train(Diabetes_binary ~ ., 
                    data = diabetes_train, 
                    method = "glm", 
                    family = binomial, 
                    trControl = train_control, 
                    metric = "logLoss")

# Model 1: LogReg Model with all except GenHlth and Age
set.seed(13579)
model_1_LogReg <- train(Diabetes_binary ~ . -GenHlth -Age, 
                data = diabetes_train, 
                method = "glm", 
                family = binomial, 
                trControl = train_control, 
                metric = "logLoss")

# Model 2: LogReg Model with all except HighBP and HighChol
set.seed(13579)
model_2_LogReg <- train(Diabetes_binary ~ . -HighBP -HighChol, 
                data = diabetes_train, 
                method = "glm", 
                family = binomial, 
                trControl = train_control, 
                metric = "logLoss")

# Model 3: LogReg Model with all except Age
set.seed(13579)
model_3_LogReg <- train(Diabetes_binary ~ . -Age,
                data = diabetes_train, 
                method = "glm", 
                family = binomial, 
                trControl = train_control, 
                metric = "logLoss")

# Model 4: LogReg Model with all except HighChol
set.seed(13579)
model_4_LogReg <- train(Diabetes_binary ~ . -HighChol, 
                data = diabetes_train, 
                method = "glm", 
                family = binomial, 
                trControl = train_control, 
                metric = "logLoss")


# Model 5: LogReg Model with all except HighBP
set.seed(13579)
model_5_LogReg <- train(Diabetes_binary ~ . -HighBP,
                data = diabetes_train, 
                method = "glm", 
                family = binomial, 
                trControl = train_control, 
                metric = "logLoss")


# Create a data frame to store the log-loss values
log_loss_results <- data.frame(
  Model = c("Base Model", "Model 1", "Model 2", "Model 3", "Model 4", "Model 5"),
  LogLoss = c(
    min(base_model_LogReg$results$logLoss),
    min(model_1_LogReg$results$logLoss),
    min(model_2_LogReg$results$logLoss),
    min(model_3_LogReg$results$logLoss),
    min(model_4_LogReg$results$logLoss),
    min(model_5_LogReg$results$logLoss)
  )
)

# Print the log-loss results
print(log_loss_results)

# Find the model with the minimum log-loss
best_model <- log_loss_results[which.min(log_loss_results$LogLoss), ]

# Print the best model
print(best_model)

```

### Classification Tree

```{r}

set.seed(13579)
# Base tree model with no interactions
base_model_tree <- rpart(Diabetes_binary ~ ., 
                         data = diabetes_train, 
                         method = "class", 
                         control = rpart.control(minbucket = 20, cp = 0.002))

# Plot the tree
base_model_rpart_plot <- rpart.plot(base_model_tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Classification Tree for Diabetes Data Base Model")
# Predict on the test set
base_model_predictions <- predict(base_model_tree, diabetes_test, type = "class")
# Confusion matrix to evaluate the model
base_model_conf_matrix <- confusionMatrix(base_model_predictions, diabetes_test$Diabetes_binary)

set.seed(13579)
# Model 1: LogReg Model with all except GenHlth and Age
model_1_tree <- rpart(Diabetes_binary ~ . -GenHlth -Age, data = diabetes_train, method = "class", control = rpart.control(minbucket = 20, cp = 0.002))
# Plot the tree
model_1_rpart_plot <- rpart.plot(model_1_tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Classification Tree for Diabetes Data Model 1")
# Predict on the test set
model_1_predictions <- predict(model_1_tree, diabetes_test, type = "class")
# Confusion matrix to evaluate the model
model_1_conf_matrix <- confusionMatrix(model_1_predictions, diabetes_test$Diabetes_binary)

set.seed(13579)
# Model 2: LogReg Model with all except HighBP and HighChol
model_2_tree <- rpart(Diabetes_binary ~ . -HighBP -HighChol, data = diabetes_train, method = "class", control = rpart.control(minbucket = 20, cp = 0.002))
# Plot the tree
model_2_rpart_plot <- rpart.plot(model_2_tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Classification Tree for Diabetes Data Model 2")
# Predict on the test set
model_2_predictions <- predict(model_2_tree, diabetes_test, type = "class")
# Confusion matrix to evaluate the model
model_2_conf_matrix <- confusionMatrix(model_2_predictions, diabetes_test$Diabetes_binary)

set.seed(13579)
# Model 3: LogReg Model with all except Age
model_3_tree <- rpart(Diabetes_binary ~ . -Age, data = diabetes_train, method = "class", control = rpart.control(minbucket = 20, cp = 0.002))
# Plot the tree
model_3_rpart_plot <- rpart.plot(model_3_tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Classification Tree for Diabetes Data Model 3")
# Predict on the test set
model_3_predictions <- predict(model_3_tree, diabetes_test, type = "class")
# Confusion matrix to evaluate the model
model_3_conf_matrix <- confusionMatrix(model_3_predictions, diabetes_test$Diabetes_binary)

set.seed(13579)
# Model 4: LogReg Model with all except HighChol
model_4_tree <- rpart(Diabetes_binary ~ . -HighChol, data = diabetes_train, method = "class", control = rpart.control(minbucket = 20, cp = 0.002))
# Plot the tree
model_4_rpart_plot <- rpart.plot(model_4_tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Classification Tree for Diabetes Data Model 4")
# Predict on the test set
model_4_predictions <- predict(model_4_tree, diabetes_test, type = "class")
# Confusion matrix to evaluate the model
model_4_conf_matrix <- confusionMatrix(model_4_predictions, diabetes_test$Diabetes_binary)

set.seed(13579)
# Model 5: LogReg Model with all except HighBP
model_5_tree <- rpart(Diabetes_binary ~ . -HighBP, data = diabetes_train, method = "class", control = rpart.control(minbucket = 20, cp = 0.002))
# Plot the tree
model_5_rpart_plot <- rpart.plot(model_5_tree, type = 2, extra = 104, fallen.leaves = TRUE, main = "Classification Tree for Diabetes Data Model 5")
# Predict on the test set
model_5_predictions <- predict(model_5_tree, diabetes_test, type = "class")
# Confusion matrix to evaluate the model
model_5_conf_matrix <- confusionMatrix(model_5_predictions, diabetes_test$Diabetes_binary)
```

```{r}
# Function to calculate evaluation metrics from a confusion matrix
calculate_metrics <- function(conf_matrix) {
  accuracy <- conf_matrix$overall['Accuracy']
  precision <- conf_matrix$byClass['Pos Pred Value']
  recall <- conf_matrix$byClass['Sensitivity']
  f1 <- 2 * ((precision * recall) / (precision + recall))
  
  return(c(accuracy = accuracy, precision = precision, recall = recall, f1 = f1))
}

# Evaluate metrics for each model
base_model_metrics <- calculate_metrics(base_model_conf_matrix)
model_1_metrics <- calculate_metrics(model_1_conf_matrix)
model_2_metrics <- calculate_metrics(model_2_conf_matrix)
model_3_metrics <- calculate_metrics(model_3_conf_matrix)
model_4_metrics <- calculate_metrics(model_4_conf_matrix)
model_5_metrics <- calculate_metrics(model_5_conf_matrix)

# Combine metrics into a data frame for comparison
metrics_df <- data.frame(
  Model = c("Base Model", "Model 1", "Model 2", "Model 3", "Model 4", "Model 5"),
  Accuracy = c(base_model_metrics["accuracy.Accuracy"], model_1_metrics["accuracy.Accuracy"], 
               model_2_metrics["accuracy.Accuracy"], model_3_metrics["accuracy.Accuracy"], 
               model_4_metrics["accuracy.Accuracy"], model_5_metrics["accuracy.Accuracy"]),
  Precision = c(base_model_metrics["precision.Pos Pred Value"], model_1_metrics["precision.Pos Pred Value"], 
                model_2_metrics["precision.Pos Pred Value"], model_3_metrics["precision.Pos Pred Value"], 
                model_4_metrics["precision.Pos Pred Value"], model_5_metrics["precision.Pos Pred Value"]),
  Recall = c(base_model_metrics["recall.Sensitivity"], model_1_metrics["recall.Sensitivity"], 
             model_2_metrics["recall.Sensitivity"], model_3_metrics["recall.Sensitivity"], 
             model_4_metrics["recall.Sensitivity"], model_5_metrics["recall.Sensitivity"]),
  F1_Score = c(base_model_metrics["f1.Pos Pred Value"], model_1_metrics["f1.Pos Pred Value"], 
               model_2_metrics["f1.Pos Pred Value"], model_3_metrics["f1.Pos Pred Value"], 
               model_4_metrics["f1.Pos Pred Value"], model_5_metrics["f1.Pos Pred Value"])
)
```

```{r}
# Identify the model names and metrics
model_names <- metrics_df$Model
metrics <- metrics_df[, -1]  # Exclude the Model column

# Find the maximum values and corresponding model names for each metric
max_accuracy_value <- max(metrics$Accuracy, na.rm = TRUE)
max_accuracy_model <- model_names[which.max(metrics$Accuracy)]

max_precision_value <- max(metrics$Precision, na.rm = TRUE)
max_precision_model <- model_names[which.max(metrics$Precision)]

max_recall_value <- max(metrics$Recall, na.rm = TRUE)
max_recall_model <- model_names[which.max(metrics$Recall)]

max_f1_value <- max(metrics$F1_Score, na.rm = TRUE)
max_f1_model <- model_names[which.max(metrics$F1_Score)]

# Combine the results into a data frame
max_metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "F1_Score"),
  Model = c(max_accuracy_model, max_precision_model, max_recall_model, max_f1_model),
  Max_Value = c(max_accuracy_value, max_precision_value, max_recall_value, max_f1_value)
)

# Print the resulting data frame
print(max_metrics_df)
```

### Random Forest
```{r}
set.seed(13579)

#Fit the randomForest models
base_model_rf <- randomForest(Diabetes_binary ~ ., data = diabetes_train, mtry = ncol(diabetes_train) / 3, ntree = 100, importance = TRUE)
model_1_rf <- randomForest(Diabetes_binary ~ . -GenHlth -Age, data = diabetes_train, mtry = ncol(diabetes_train) / 3, ntree = 100, importance = TRUE)
model_2_rf <- randomForest(Diabetes_binary ~ . -HighBP -HighChol, data = diabetes_train, mtry = ncol(diabetes_train) / 3, ntree = 100, importance = TRUE)
model_3_rf <- randomForest(Diabetes_binary ~ . -Age, data = diabetes_train, mtry = ncol(diabetes_train) / 3, ntree = 100, importance = TRUE)
model_4_rf <- randomForest(Diabetes_binary ~ . -HighChol, data = diabetes_train, mtry = ncol(diabetes_train) / 3, ntree = 100, importance = TRUE)
model_5_rf <- randomForest(Diabetes_binary ~ . -HighBP, data = diabetes_train, mtry = ncol(diabetes_train) / 3, ntree = 100, importance = TRUE)

# Predict probabilities for the test set
base_model_rfProbs <- predict(base_model_rf, newdata = dplyr::select(diabetes_test, -Diabetes_binary), type = "prob")
model_1_rfProbs <- predict(model_1_rf, newdata = dplyr::select(diabetes_test, -Diabetes_binary), type = "prob")
model_2_rfProbs <- predict(model_2_rf, newdata = dplyr::select(diabetes_test, -Diabetes_binary), type = "prob")
model_3_rfProbs <- predict(model_3_rf, newdata = dplyr::select(diabetes_test, -Diabetes_binary), type = "prob")
model_4_rfProbs <- predict(model_4_rf, newdata = dplyr::select(diabetes_test, -Diabetes_binary), type = "prob")
model_5_rfProbs <- predict(model_5_rf, newdata = dplyr::select(diabetes_test, -Diabetes_binary), type = "prob")

# Convert probabilities to predicted classes
base_model_rfPred <- ifelse(base_model_rfProbs[, 2] > 0.5, "X1", "X0")
model_1_rfPred <- ifelse(model_1_rfProbs[, 2] > 0.5, "X1", "X0")
model_2_rfPred <- ifelse(model_2_rfProbs[, 2] > 0.5, "X1", "X0")
model_3_rfPred <- ifelse(model_3_rfProbs[, 2] > 0.5, "X1", "X0")
model_4_rfPred <- ifelse(model_4_rfProbs[, 2] > 0.5, "X1", "X0")
model_5_rfPred <- ifelse(model_5_rfProbs[, 2] > 0.5, "X1", "X0")

# Create confusion matrices
conf_matrix_base <- confusionMatrix(factor(base_model_rfPred, levels = levels(diabetes_test$Diabetes_binary)), diabetes_test$Diabetes_binary)
conf_matrix_1 <- confusionMatrix(factor(model_1_rfPred, levels = levels(diabetes_test$Diabetes_binary)), diabetes_test$Diabetes_binary)
conf_matrix_2 <- confusionMatrix(factor(model_2_rfPred, levels = levels(diabetes_test$Diabetes_binary)), diabetes_test$Diabetes_binary)
conf_matrix_3 <- confusionMatrix(factor(model_3_rfPred, levels = levels(diabetes_test$Diabetes_binary)), diabetes_test$Diabetes_binary)
conf_matrix_4 <- confusionMatrix(factor(model_4_rfPred, levels = levels(diabetes_test$Diabetes_binary)), diabetes_test$Diabetes_binary)
conf_matrix_5 <- confusionMatrix(factor(model_5_rfPred, levels = levels(diabetes_test$Diabetes_binary)), diabetes_test$Diabetes_binary)

# Extract accuracy
accuracy_base <- conf_matrix_base$overall['Accuracy']
accuracy_1 <- conf_matrix_1$overall['Accuracy']
accuracy_2 <- conf_matrix_2$overall['Accuracy']
accuracy_3 <- conf_matrix_3$overall['Accuracy']
accuracy_4 <- conf_matrix_4$overall['Accuracy']
accuracy_5 <- conf_matrix_5$overall['Accuracy']

# Create a dataframe with the accuracies
accuracy_df <- data.frame(
  Model = c("Base Model", "Model 1", "Model 2", "Model 3", "Model 4", "Model 5"),
  Accuracy = c(accuracy_base, accuracy_1, accuracy_2, accuracy_3, accuracy_4, accuracy_5)
)

# Print the dataframe
print(accuracy_df)
```

### Final Model Selection
```{r}
# Print the log-loss results
print(log_loss_results)

# Print the resulting data frame
print(max_metrics_df)

# Print the dataframe of the random forest models
print(accuracy_df)

```


#### **Appendix:  Variable description**
[@cdc2015]\
**Diabetes_binary**\
_NOTE:  The options present are different from the source data._\
0 = no diabetes/prediabetes\
1 = diabetes\

**HighBP**\
0 = no high BP\
1 = high BP\

**HighChol**\
0 = no high cholesterol\
1 = high cholesterol\

**CholCheck**\
0 = no cholesterol check in 5 years\
1 = cholesterol check in 5 years\

**BMI**\
Continuous Data\

**Smoker**\
To the question:  Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes] \
0 = no\
1 = yes\

**Stroke**\
To the question:  (Ever told) you had a stroke.\
0 = no\
1 = yes\

**HeartDiseaseorAttack**\
Coronary heart disease (CHD) or myocardial infarction (MI)\
0 = no\
1 = yes\

**PhysActivity**\
Physical activity in past 30 days (not including job)\
0 = no\
1 = yes\

**Fruits**\
Consume Fruit 1 or more times per day\
0 = no\
1 = yes\

**Veggies**\
Consume Vegetables 1 or more times per day\
0 = no\
1 = yes\

**HvyAlcoholConsump**\
(adult men >=14 drinks per week and adult women>=7 drinks per week)\
0 = no\
1 = yes\

**AnyHealthcare**\
Have any kind of health care coverage, including health insurance, prepaid plans such as HMO, etc.\
0 = no\
1 = yes\

**NoDocbcCost**\
To the question:  Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?\
0 = no\
1 = yes\

**GenHlth**\
To the question:  Would you say that in general your health is: scale 1-5\
1 = excellent\
2 = very good\
3 = good\
4 = fair\
5 = poor\

**MentHlth**\
Days of poor mental health scale 1-30 days\

**PhysHlth**\
physical illness or injury days in past 30 days scale 1-30\

**DiffWalk**\
Do you have serious difficulty walking or climbing stairs?\
0 = no\
1 = yes\

**Sex**\
0 = female\
1 = male\

**Age**\
_NOTE:  The options present are different from the source data._\
1 = Age 18 to 24 Respondents with reported age between 18 and 24 years (18 <= AGE <= 24)\
2 = Age 25 to 29 Respondents with reported age between 25 and 29 years (25 <= AGE <= 29)\
3 = Age 30 to 34 Respondents with reported age between 30 and 34 years (30 <= AGE <= 34)\
4 = Age 35 to 39 Respondents with reported age between 35 and 39 years (35 <= AGE <= 39)\
5 = Age 40 to 44 Respondents with reported age between 40 and 44 years (40 <= AGE <= 44)\
6 = Age 45 to 49 Respondents with reported age between 45 and 49 years (45 <= AGE <= 49)\
7 = Age 50 to 54 Respondents with reported age between 50 and 54 years (50 <= AGE <= 54)\
8 = Age 55 to 59 Respondents with reported age between 55 and 59 years (55 <= AGE <= 59)\
9 = Age 60 to 64 Respondents with reported age between 60 and 64 years (60 <= AGE <= 64)\
10 = Age 65 to 69 Respondents with reported age between 65 and 69 years (65 <= AGE <= 69)\
11 = Age 70 to 74 Respondents with reported age between 70 and 74 years (70 <= AGE <= 74)\
12 = Age 75 to 79 Respondents with reported age between 75 and 79 years (75 <= AGE <= 79)\
13 = Age 80 or older Respondents with reported age between 80 and 99 years (80 <= AGE <= 99)\

**Education**\
_NOTE:  The options present are different from the source data._\
1 = Never attended school or only kindergarten\
2 = Grades 1 through 8 (Elementary)\
3 = Grades 9 through 11 (Some high school)\
4 = Grade 12 or GED (High school graduate)\
5 = College 1 year to 3 years (Some college or technical school)\
6 = College 4 years or more (College graduate)\

**Income**\
_NOTE:  The options present are different from the source data._\
1 = Less than $10,000\
2 = Less than $15,000 ($10,000 to less than $15,000)\
3 = Less than $20,000 ($15,000 to less than $20,000)\
4 = Less than $25,000 ($20,000 to less than $25,000)\
5 = Less than $35,000 ($25,000 to less than $35,000)\
6 = Less than $50,000 ($35,000 to less than $50,000)\
7 = Less than $75,000 ($50,000 to less than $75,000)\
8 = $75,000 or more\

# REMINDER TO DROP THE LINK TO THE EDA.QMD

# References