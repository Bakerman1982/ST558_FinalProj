[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Diabetes Health Indicators - Modeling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(ggplot2)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(randomForest)\n\n\nDiabetes Health Indicators Modeling\nAfter conducting an exploratory data analysis (EDA), our next objective is for inference about this dataset and to posit several models, test them, and determine which ones best describe our data. Key variables of interest include high cholesterol, high blood pressure, general health and age. Utilizing logistic regression modeling, classification tree modeling, and random forest modeling; we will identify the best-performing model from each category and compare their performance on the test data to determine the overall best model.\n\n#Read in the dataset, Factorize the binary variables for better downstream analysis and remove BMI observations 50.  Combining in this way saves substantial amount of memory in the R environment. \ndiabetes_data_clean &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\") %&gt;%\n  mutate_at(vars(Diabetes_binary, HighBP, HighChol, CholCheck, Smoker, \n                 Stroke, HeartDiseaseorAttack, PhysActivity, Fruits, \n                 Veggies, HvyAlcoholConsump, AnyHealthcare, NoDocbcCost, \n                 DiffWalk, Sex), as.factor) %&gt;%\n  filter(BMI &lt; 50)\n\n\n\nData Preparation for Modeling\nUsing the caret package syntax, I have split the data into train and test at a 70:30 ratio respectively. This is done as a mean of testing our predictions on testing, or unseen data. This acts as a baseline to which we can refer back to and gauge model performance.\n\n#Split the data using caret package syntax\nset.seed(13579)\ndiabetesIndex &lt;- createDataPartition(diabetes_data_clean$Diabetes_binary, p = 0.7, list = FALSE)\ndiabetes_train &lt;- diabetes_data_clean[diabetesIndex, ] #Count = 175,794\ndiabetes_test &lt;- diabetes_data_clean[-diabetesIndex, ] #Count = 75,339\n\n#I had some problems downstream while planning and executing code to build models. R does not like when I run train() on a response variable with strictly numeric factors.  The following code corrects that issue by changing the response values from \"0\" and \"1\" to \"X0\" and \"X1\".  Henceforth, \"X0\" will indicate patients who do not have a diabetes diagnosis while an \"X1\" will affirm a diabetes diagnosis. \n\n# Rename levels to valid variable names\nlevels(diabetes_train$Diabetes_binary) &lt;- make.names(levels(diabetes_train$Diabetes_binary))\nlevels(diabetes_test$Diabetes_binary) &lt;- make.names(levels(diabetes_test$Diabetes_binary))\n\n\n\nLogistic Regression Modeling and Log Loss\nA Logistic Regression Model is a modeling technique designed for binary classification, where the target variable is categorical with two possible outcomes (e.g., 0 and 1, true and false, or success and failure). This model estimates the likelihood that a particular input falls into one of the categories. Logistic Regression is a great option for modeling with this specific dataset since we are working in binary data while we also want to makes no assumptions about the distribution of our data. Another strength is that LogLoss compares prediction probabilities to binary output; and evaluates probability estimates which make for a more robust measurement method over accuracy for probabilistic models. Interpretation is made simpler with log-odd to one-unit change predictor variable.\nUsing linear modeling might not be appropriate for this data. Logistic regression modeling would be a better alternative. In the plot below, notice that the linear regression line does not fit our data well for a variable BMI. The entire linear regression line completely avoids the positive diabetes diagnosis points. This is due to the overwhelmingly disproportion of non-diabetic responses.\n\nggplot(diabetes_data_clean, aes(x = BMI, y = Diabetes_binary)) + \n  geom_jitter() + \n  geom_smooth(method = \"lm\", aes(group = 1))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCaret contains a trainControl() function that allows the setting of modeling parameters\n\n# Define the control using a 5-fold cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", number = 5, summaryFunction = mnLogLoss, classProbs = TRUE)\n#NOTES:  classProbs has to be TRUE for mnLogLoss to work.\n#Source:  https://stackoverflow.com/questions/59669490/error-with-caret-and-summaryfunction-mnlogloss-columns-consistent-with-lev\n\nHere is the setup for performing a logistic regression modeling on three candidate models I am interested in testing. These same formula models will be used in classification and in random forest modeling.\n\n# Model 1: LogReg Model with all except GenHlth and Age\nset.seed(13579)\nmodel_1_LogReg &lt;- train(Diabetes_binary ~ . -GenHlth -Age, \n                data = diabetes_train, \n                method = \"glm\", \n                family = binomial, \n                trControl = train_control, \n                metric = \"logLoss\")\n\n# Model 2: LogReg Model with all except HighBP and HighChol\nset.seed(13579)\nmodel_2_LogReg &lt;- train(Diabetes_binary ~ . -HighBP -HighChol, \n                data = diabetes_train, \n                method = \"glm\", \n                family = binomial, \n                trControl = train_control, \n                metric = \"logLoss\")\n\n# Model 3: LogReg Model with all except HighChol\nset.seed(13579)\nmodel_3_LogReg &lt;- train(Diabetes_binary ~ . -HighChol, \n                data = diabetes_train, \n                method = \"glm\", \n                family = binomial, \n                trControl = train_control, \n                metric = \"logLoss\")\n\n\n# Create a data frame to store the log-loss values\nlog_loss_results &lt;- data.frame(\n  Model = c(\"Model 1\", \"Model 2\", \"Model 3\"),\n  LogLoss = c(\n    min(model_1_LogReg$results$logLoss),\n    min(model_2_LogReg$results$logLoss),\n    min(model_3_LogReg$results$logLoss)\n  )\n)\n\n# Print the log-loss results.\n# Lower Log Loss indicates better model performance in that lower log loss can be interpreted as less of a difference between the actual classes and those that were predicted.\nprint(log_loss_results)\n\n    Model   LogLoss\n1 Model 1 0.3286191\n2 Model 2 0.3252869\n3 Model 3 0.3190218\n\n\n\n\nClassification Tree\nA classification tree model, commonly referred to as a decision tree, is a predictive technique used for classification problems. It operates by dividing the dataset into smaller subsets based on the values of input features, creating a tree-like structure. In this structure, each internal node signifies a decision based on a particular feature, each branch indicates the result of that decision, and each leaf node denotes a class label. Binary data works well with tree data since trees are naturally forked in halves. This is visually intuitive in that it tells the story of the decision at the node. Although we handled outliers earlier, trees are less sensitive to them. Like logistic regression, classification trees make no assumptions about the distributionâ€“linearity is not consequential to this model.\n\nset.seed(13579)\n\n# Model 1: LogReg Model with all except GenHlth and Age\ncp_values_model_1 &lt;- seq(0.000, 0.004, by = 0.001)\nresults_model_1 &lt;- data.frame(CP = numeric(), Accuracy = numeric())\n\n# Loop through each cp value\nfor (cp in cp_values_model_1) {\n  model_1_tree &lt;- rpart(Diabetes_binary ~ . -GenHlth -Age, data = diabetes_train, method = \"class\", control = rpart.control(minbucket = 20, cp = cp))\n  \n  # Predict on the test set\n  model_1_predictions &lt;- predict(model_1_tree, diabetes_test, type = \"class\")\n  \n  # Confusion matrix to evaluate the model\n  model_1_conf_matrix &lt;- confusionMatrix(model_1_predictions, diabetes_test$Diabetes_binary)\n  \n  # Store accuracy\n  accuracy &lt;- model_1_conf_matrix$overall['Accuracy']\n  results_model_1 &lt;- rbind(results_model_1, data.frame(CP = cp, Accuracy = accuracy))\n}\n\n# Find the best cp based on maximum accuracy\nbest_cp_model_1 &lt;- results_model_1$CP[which.max(results_model_1$Accuracy)]\nprint(paste0(\"Optimal Complexity Parameter: \", best_cp_model_1))\n\n[1] \"Optimal Complexity Parameter: 0.001\"\n\n# Fit the final model using the best cp value\nmodel_1_final_tree &lt;- rpart(Diabetes_binary ~ . -GenHlth -Age, data = diabetes_train, method = \"class\", control = rpart.control(minbucket = 20, cp = best_cp_model_1))\n\n# Plot the tree\npar(mar = c(1, 1, 1, 1))  # Adjust margins (bottom, left, top, right)\nmodel_1_rpart_plot &lt;- rpart.plot(model_1_final_tree, type = 2, extra = 104, fallen.leaves = TRUE, main = \"Classification Tree for Diabetes Data Model 1\")\n\n\n\n\n\n\n\n# Print the final confusion matrix for the best model\nmodel_1_final_predictions &lt;- predict(model_1_final_tree, diabetes_test, type = \"class\")\nmodel_1_final_conf_matrix &lt;- confusionMatrix(model_1_final_predictions, diabetes_test$Diabetes_binary)\nprint(model_1_final_conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    X0    X1\n        X0 64471  9603\n        X1   520   745\n                                          \n               Accuracy : 0.8656          \n                 95% CI : (0.8632, 0.8681)\n    No Information Rate : 0.8626          \n    P-Value [Acc &gt; NIR] : 0.008606        \n                                          \n                  Kappa : 0.1014          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.99200         \n            Specificity : 0.07199         \n         Pos Pred Value : 0.87036         \n         Neg Pred Value : 0.58893         \n             Prevalence : 0.86265         \n         Detection Rate : 0.85575         \n   Detection Prevalence : 0.98321         \n      Balanced Accuracy : 0.53200         \n                                          \n       'Positive' Class : X0              \n                                          \n\n\n\nset.seed(13579)\n\n# Model 2: LogReg Model with all except HighBP and HighChol\ncp_values_model_2 &lt;- seq(0.000, 0.004, by = 0.001)\nresults_model_2 &lt;- data.frame(CP = numeric(), Accuracy = numeric())\n\n# Loop through each cp value\nfor (cp in cp_values_model_2) {\n  model_2_tree &lt;- rpart(Diabetes_binary ~ . -HighBP -HighChol, data = diabetes_train, method = \"class\", control = rpart.control(minbucket = 20, cp = cp))\n  \n  # Predict on the test set\n  model_2_predictions &lt;- predict(model_2_tree, diabetes_test, type = \"class\")\n  \n  # Confusion matrix to evaluate the model\n  model_2_conf_matrix &lt;- confusionMatrix(model_2_predictions, diabetes_test$Diabetes_binary)\n  \n  # Store accuracy\n  accuracy &lt;- model_2_conf_matrix$overall['Accuracy']\n  results_model_2 &lt;- rbind(results_model_2, data.frame(CP = cp, Accuracy = accuracy))\n}\n\n# Find the best cp based on maximum accuracy\nbest_cp_model_2 &lt;- results_model_2$CP[which.max(results_model_2$Accuracy)]\nprint(paste0(\"Optimal Complexity Parameter: \", best_cp_model_2))\n\n[1] \"Optimal Complexity Parameter: 0.001\"\n\n# Fit the final model using the best cp value\nmodel_2_final_tree &lt;- rpart(Diabetes_binary ~ . -HighBP -HighChol, data = diabetes_train, method = \"class\", control = rpart.control(minbucket = 20, cp = best_cp_model_2))\n\n# Plot the tree\npar(mar = c(1, 1, 1, 1))  # Adjust margins (bottom, left, top, right)\nmodel_2_rpart_plot &lt;- rpart.plot(model_2_final_tree, type = 2, extra = 104, fallen.leaves = TRUE, main = \"Classification Tree for Diabetes Data Model 2\")\n\n\n\n\n\n\n\n# Print the final confusion matrix for the best model\nmodel_2_final_predictions &lt;- predict(model_2_final_tree, diabetes_test, type = \"class\")\nmodel_2_final_conf_matrix &lt;- confusionMatrix(model_2_final_predictions, diabetes_test$Diabetes_binary)\nprint(model_2_final_conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    X0    X1\n        X0 64434  9586\n        X1   557   762\n                                          \n               Accuracy : 0.8654          \n                 95% CI : (0.8629, 0.8678)\n    No Information Rate : 0.8626          \n    P-Value [Acc &gt; NIR] : 0.01503         \n                                          \n                  Kappa : 0.1028          \n                                          \n Mcnemar's Test P-Value : &lt; 2e-16         \n                                          \n            Sensitivity : 0.99143         \n            Specificity : 0.07364         \n         Pos Pred Value : 0.87049         \n         Neg Pred Value : 0.57771         \n             Prevalence : 0.86265         \n         Detection Rate : 0.85525         \n   Detection Prevalence : 0.98249         \n      Balanced Accuracy : 0.53253         \n                                          \n       'Positive' Class : X0              \n                                          \n\n\n\nset.seed(13579)\n\n# Model 3: LogReg Model with all except HighChol\ncp_values_model_3 &lt;- seq(0.000, 0.004, by = 0.001)\nresults_model_3 &lt;- data.frame(CP = numeric(), Accuracy = numeric())\n\n# Loop through each cp value\nfor (cp in cp_values_model_3) {\n  model_3_tree &lt;- rpart(Diabetes_binary ~ . -HighChol, data = diabetes_train, method = \"class\", control = rpart.control(minbucket = 20, cp = cp))\n  \n  # Predict on the test set\n  model_3_predictions &lt;- predict(model_3_tree, diabetes_test, type = \"class\")\n  \n  # Confusion matrix to evaluate the model\n  model_3_conf_matrix &lt;- confusionMatrix(model_3_predictions, diabetes_test$Diabetes_binary)\n  \n  # Store accuracy\n  accuracy &lt;- model_3_conf_matrix$overall['Accuracy']\n  results_model_3 &lt;- rbind(results_model_3, data.frame(CP = cp, Accuracy = accuracy))\n}\n\n# Find the best cp based on maximum accuracy\nbest_cp_model_3 &lt;- results_model_3$CP[which.max(results_model_3$Accuracy)]\nprint(paste0(\"Optimal Complexity Parameter: \", best_cp_model_3))\n\n[1] \"Optimal Complexity Parameter: 0.002\"\n\n# Fit the final model using the best cp value\nmodel_3_final_tree &lt;- rpart(Diabetes_binary ~ . -HighChol, data = diabetes_train, method = \"class\", control = rpart.control(minbucket = 20, cp = best_cp_model_3))\n\n# Plot the tree\npar(mar = c(1, 1, 1, 1))  # Adjust margins (bottom, left, top, right)\nmodel_3_rpart_plot &lt;- rpart.plot(model_3_final_tree, type = 2, extra = 104, fallen.leaves = TRUE, main = \"Classification Tree for Diabetes Data Model 3\")\n\n\n\n\n\n\n\n# Print the final confusion matrix for the best model\nmodel_3_final_predictions &lt;- predict(model_3_final_tree, diabetes_test, type = \"class\")\nmodel_3_final_conf_matrix &lt;- confusionMatrix(model_3_final_predictions, diabetes_test$Diabetes_binary)\nprint(model_3_final_conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    X0    X1\n        X0 64327  9380\n        X1   664   968\n                                          \n               Accuracy : 0.8667          \n                 95% CI : (0.8642, 0.8691)\n    No Information Rate : 0.8626          \n    P-Value [Acc &gt; NIR] : 0.0006312       \n                                          \n                  Kappa : 0.129           \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.98978         \n            Specificity : 0.09354         \n         Pos Pred Value : 0.87274         \n         Neg Pred Value : 0.59314         \n             Prevalence : 0.86265         \n         Detection Rate : 0.85383         \n   Detection Prevalence : 0.97834         \n      Balanced Accuracy : 0.54166         \n                                          \n       'Positive' Class : X0              \n                                          \n\n\n\n# Function to calculate evaluation metrics from a confusion matrix\ncalculate_metrics &lt;- function(conf_matrix) {\n  accuracy &lt;- conf_matrix$overall['Accuracy']\n  precision &lt;- conf_matrix$byClass['Pos Pred Value']\n  recall &lt;- conf_matrix$byClass['Sensitivity']\n  f1 &lt;- 2 * ((precision * recall) / (precision + recall))\n  return(c(accuracy = accuracy, precision = precision, recall = recall, f1 = f1))\n}\n\n# Evaluate metrics for each model\nmodel_1_metrics &lt;- calculate_metrics(model_1_final_conf_matrix)\nmodel_2_metrics &lt;- calculate_metrics(model_2_final_conf_matrix)\nmodel_3_metrics &lt;- calculate_metrics(model_3_final_conf_matrix)\n\n# Combine metrics into a data frame for comparison\nmetrics_df &lt;- data.frame(\n  Model = c(\"Model_1\", \"Model_2\", \"Model_3\"),\n  Accuracy = c(model_1_metrics[\"accuracy.Accuracy\"], \n               model_2_metrics[\"accuracy.Accuracy\"], \n               model_3_metrics[\"accuracy.Accuracy\"]),\n  Precision = c(model_1_metrics[\"precision.Pos Pred Value\"], \n                model_2_metrics[\"precision.Pos Pred Value\"], \n                model_3_metrics[\"precision.Pos Pred Value\"]),\n  Recall = c(model_1_metrics[\"recall.Sensitivity\"], \n             model_2_metrics[\"recall.Sensitivity\"], \n             model_3_metrics[\"recall.Sensitivity\"]),\n  F1_Score = c(model_1_metrics[\"f1.Pos Pred Value\"], \n               model_2_metrics[\"f1.Pos Pred Value\"], \n               model_3_metrics[\"f1.Pos Pred Value\"])\n)\n\n# Identify the model names and metrics\nmodel_names &lt;- metrics_df$Model\nmetrics &lt;- metrics_df[, -1]  # Exclude the Model column\n\n# Find the maximum values and corresponding model names for each metric\nmax_accuracy_value &lt;- max(metrics$Accuracy, na.rm = TRUE)\nmax_accuracy_model &lt;- model_names[which.max(metrics$Accuracy)]\n\nmax_precision_value &lt;- max(metrics$Precision, na.rm = TRUE)\nmax_precision_model &lt;- model_names[which.max(metrics$Precision)]\n\nmax_recall_value &lt;- max(metrics$Recall, na.rm = TRUE)\nmax_recall_model &lt;- model_names[which.max(metrics$Recall)]\n\nmax_f1_value &lt;- max(metrics$F1_Score, na.rm = TRUE)\nmax_f1_model &lt;- model_names[which.max(metrics$F1_Score)]\n\n# Combine the results into a data frame\nmax_metrics_df &lt;- data.frame(\n  Metric = c(\"Accuracy\", \"Precision\", \"Recall\", \"F1_Score\"),\n  Model = c(max_accuracy_model, max_precision_model, max_recall_model, max_f1_model),\n  Max_Value = c(max_accuracy_value, max_precision_value, max_recall_value, max_f1_value)\n)\n\n# Print the resulting data frame\nprint(metrics_df)\n\n    Model  Accuracy Precision    Recall  F1_Score\n1 Model_1 0.8656340 0.8703594 0.9919989 0.9272067\n2 Model_2 0.8653685 0.8704945 0.9914296 0.9270346\n3 Model_3 0.8666826 0.8727394 0.9897832 0.9275837\n\n\n\n\nRandom Forest\nA random forest is a powerful machine learning technique that utilizes a collection of decision trees to enhance classification performance and minimize the risk of overfitting. In contrast to a single decision tree, which can be overly sensitive to variations in the training data, a random forest builds numerous decision trees by randomly selecting subsets of both the data and the features for each tree. Each tree independently generates its predictions, and the final classification is determined across all trees. This method increases the overall stability and accuracy of the model by reducing the likelihood of capturing noise that may exist in any single dataset.\n\nset.seed(13579)\n\n#Fit the randomForest models\nmodel_1_rf &lt;- randomForest(Diabetes_binary ~ . -GenHlth -Age, data = diabetes_train, mtry = ncol(diabetes_train) / 3, ntree = 50, importance = TRUE)\nmodel_2_rf &lt;- randomForest(Diabetes_binary ~ . -HighBP -HighChol, data = diabetes_train, mtry = ncol(diabetes_train) / 3, ntree = 50, importance = TRUE)\nmodel_3_rf &lt;- randomForest(Diabetes_binary ~ . -HighChol, data = diabetes_train, mtry = ncol(diabetes_train) / 3, ntree = 50, importance = TRUE)\n\n# Predict probabilities for the test set\nmodel_1_rfProbs &lt;- predict(model_1_rf, newdata = dplyr::select(diabetes_test, -Diabetes_binary), type = \"prob\")\nmodel_2_rfProbs &lt;- predict(model_2_rf, newdata = dplyr::select(diabetes_test, -Diabetes_binary), type = \"prob\")\nmodel_3_rfProbs &lt;- predict(model_3_rf, newdata = dplyr::select(diabetes_test, -Diabetes_binary), type = \"prob\")\n\n# Convert probabilities to predicted classes\n\nmodel_1_rfPred &lt;- ifelse(model_1_rfProbs[, 2] &gt; 0.5, \"X1\", \"X0\")\nmodel_2_rfPred &lt;- ifelse(model_2_rfProbs[, 2] &gt; 0.5, \"X1\", \"X0\")\nmodel_3_rfPred &lt;- ifelse(model_3_rfProbs[, 2] &gt; 0.5, \"X1\", \"X0\")\n\n# Create confusion matrices\nconf_matrix_1 &lt;- confusionMatrix(factor(model_1_rfPred, levels = levels(diabetes_test$Diabetes_binary)), diabetes_test$Diabetes_binary)\nconf_matrix_2 &lt;- confusionMatrix(factor(model_2_rfPred, levels = levels(diabetes_test$Diabetes_binary)), diabetes_test$Diabetes_binary)\nconf_matrix_3 &lt;- confusionMatrix(factor(model_3_rfPred, levels = levels(diabetes_test$Diabetes_binary)), diabetes_test$Diabetes_binary)\n\n# Extract accuracy\naccuracy_1 &lt;- conf_matrix_1$overall['Accuracy']\naccuracy_2 &lt;- conf_matrix_2$overall['Accuracy']\naccuracy_3 &lt;- conf_matrix_3$overall['Accuracy']\n\n# Create a dataframe with the accuracies\naccuracy_df &lt;- data.frame(\n  Model = c(\"Model 1\", \"Model 2\", \"Model 3\"),\n  Accuracy = c(accuracy_1, accuracy_2, accuracy_3)\n)\n\n# Print the dataframe\nprint(accuracy_df)\n\n    Model  Accuracy\n1 Model 1 0.8582142\n2 Model 2 0.8596079\n3 Model 3 0.8603114\n\n\n\n\nFinal Model Selection\nHere is a summary of the three different model outputs.\n\n# Print the logistic regression model results\nprint(log_loss_results)\n\n    Model   LogLoss\n1 Model 1 0.3286191\n2 Model 2 0.3252869\n3 Model 3 0.3190218\n\n\n\n# Print the classification model results\nprint(metrics_df)\n\n    Model  Accuracy Precision    Recall  F1_Score\n1 Model_1 0.8656340 0.8703594 0.9919989 0.9272067\n2 Model_2 0.8653685 0.8704945 0.9914296 0.9270346\n3 Model_3 0.8666826 0.8727394 0.9897832 0.9275837\n\n\n\n# Print the random forest model results\nprint(accuracy_df)\n\n    Model  Accuracy\n1 Model 1 0.8582142\n2 Model 2 0.8596079\n3 Model 3 0.8603114\n\n\nThe best model that fits the diabetes dataset best appears to be model 3, which excludes a high cholesterol main effect. Deciding factors were the lowest observable logloss, the highest accuracy in the random forest model and winner of three of the four metrics that measure classification models; accuracy, precision, and F1_Score. Model 3 was the least simplest compared to the other two models which had fewer main effects. I was somewhat surprised at this result.\nI had visited my primary care physician, Dr.Â Amanda Meeks, FNP, a few weeks before this project. In our discussion about my personal health, we had talked at length about how my blood pressure had crept upwards in the last check-in. My doctor mentioned that high blood pressure substantially increases the risk of having the other two conditions; as does having any one of the other medical conditions. The risks are so high that typically when diagnosed with either high blood pressure or high cholesterol, that initiates an automatic check for the other two conditions. With that in mind, I was expecting to see Model 1 perform better than it did with the inclusion of high cholesterol.\n\nAppendix: Variable description\n(Disease Control & Prevention, 2015)\nDiabetes_binary\nNOTE: The options present are different from the source data.\n0 = no diabetes/prediabetes\n1 = diabetes\n\nHighBP\n0 = no high BP\n1 = high BP\n\nHighChol\n0 = no high cholesterol\n1 = high cholesterol\n\nCholCheck\n0 = no cholesterol check in 5 years\n1 = cholesterol check in 5 years\n\nBMI\nContinuous Data\n\nSmoker\nTo the question: Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes]\n0 = no\n1 = yes\n\nStroke\nTo the question: (Ever told) you had a stroke.\n0 = no\n1 = yes\n\nHeartDiseaseorAttack\nCoronary heart disease (CHD) or myocardial infarction (MI)\n0 = no\n1 = yes\n\nPhysActivity\nPhysical activity in past 30 days (not including job)\n0 = no\n1 = yes\n\nFruits\nConsume Fruit 1 or more times per day\n0 = no\n1 = yes\n\nVeggies\nConsume Vegetables 1 or more times per day\n0 = no\n1 = yes\n\nHvyAlcoholConsump\n(adult men &gt;=14 drinks per week and adult women&gt;=7 drinks per week)\n0 = no\n1 = yes\n\nAnyHealthcare\nHave any kind of health care coverage, including health insurance, prepaid plans such as HMO, etc.\n0 = no\n1 = yes\n\nNoDocbcCost\nTo the question: Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?\n0 = no\n1 = yes\n\nGenHlth\nTo the question: Would you say that in general your health is: scale 1-5\n1 = excellent\n2 = very good\n3 = good\n4 = fair\n5 = poor\n\nMentHlth\nDays of poor mental health scale 1-30 days\n\nPhysHlth\nphysical illness or injury days in past 30 days scale 1-30\n\nDiffWalk\nDo you have serious difficulty walking or climbing stairs?\n0 = no\n1 = yes\n\nSex\n0 = female\n1 = male\n\nAge\nNOTE: The options present are different from the source data.\n1 = Age 18 to 24 Respondents with reported age between 18 and 24 years (18 &lt;= AGE &lt;= 24)\n2 = Age 25 to 29 Respondents with reported age between 25 and 29 years (25 &lt;= AGE &lt;= 29)\n3 = Age 30 to 34 Respondents with reported age between 30 and 34 years (30 &lt;= AGE &lt;= 34)\n4 = Age 35 to 39 Respondents with reported age between 35 and 39 years (35 &lt;= AGE &lt;= 39)\n5 = Age 40 to 44 Respondents with reported age between 40 and 44 years (40 &lt;= AGE &lt;= 44)\n6 = Age 45 to 49 Respondents with reported age between 45 and 49 years (45 &lt;= AGE &lt;= 49)\n7 = Age 50 to 54 Respondents with reported age between 50 and 54 years (50 &lt;= AGE &lt;= 54)\n8 = Age 55 to 59 Respondents with reported age between 55 and 59 years (55 &lt;= AGE &lt;= 59)\n9 = Age 60 to 64 Respondents with reported age between 60 and 64 years (60 &lt;= AGE &lt;= 64)\n10 = Age 65 to 69 Respondents with reported age between 65 and 69 years (65 &lt;= AGE &lt;= 69)\n11 = Age 70 to 74 Respondents with reported age between 70 and 74 years (70 &lt;= AGE &lt;= 74)\n12 = Age 75 to 79 Respondents with reported age between 75 and 79 years (75 &lt;= AGE &lt;= 79)\n13 = Age 80 or older Respondents with reported age between 80 and 99 years (80 &lt;= AGE &lt;= 99)\n\nEducation\nNOTE: The options present are different from the source data.\n1 = Never attended school or only kindergarten\n2 = Grades 1 through 8 (Elementary)\n3 = Grades 9 through 11 (Some high school)\n4 = Grade 12 or GED (High school graduate)\n5 = College 1 year to 3 years (Some college or technical school)\n6 = College 4 years or more (College graduate)\n\nIncome\nNOTE: The options present are different from the source data.\n1 = Less than $10,000\n2 = Less than $15,000 ($10,000 to less than $15,000)\n3 = Less than $20,000 ($15,000 to less than $20,000)\n4 = Less than $25,000 ($20,000 to less than $25,000)\n5 = Less than $35,000 ($25,000 to less than $35,000)\n6 = Less than $50,000 ($35,000 to less than $50,000)\n7 = Less than $75,000 ($50,000 to less than $75,000)\n8 = $75,000 or more\n\nLink to EDA.qmd on GitHub\n\n\n\n\n\n\n\n\n\nReferences\n\nDisease Control, C. for, & Prevention. (2015). BRFSS codebook 2015. https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf"
  }
]